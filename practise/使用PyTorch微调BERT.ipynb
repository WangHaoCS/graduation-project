{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0716671a",
   "metadata": {},
   "source": [
    "# 使用 PyTorch 微调 BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fdfefa",
   "metadata": {},
   "source": [
    "从 Hugging Face Hub 上加载预训练的 BERT 模型，然后使用 PyTorch 纯手工对其进行微调，设定如下：\n",
    "- 预训练模型：bert-base-uncased\n",
    "- 下游任务：GLUE/SST-2\n",
    "\n",
    "使用 PyTorch 微调 BERT 需要以下步骤：\n",
    "- 数据预处理：加载数据集并定义 `Dataset` 和 `DataLoader`\n",
    "- 模型定义：给 BERT 基础模型添加一个全连接层作为分类头\n",
    "- 模型微调：使用 AdamW 优化器对模型进行微调\n",
    "- 模型验证：计算模型在验证集上的准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f934786",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from transformers import (\n",
    "    BertTokenizerFast,\n",
    "    BertModel,\n",
    "    DataCollatorWithPadding,\n",
    "    set_seed\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "from typing import Callable\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915a1995",
   "metadata": {},
   "source": [
    "一些超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f22d08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 1\n",
    "learning_rate = 5e-5\n",
    "device = \"cuda:1\"\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfa044e",
   "metadata": {},
   "source": [
    "## 加载数据集\n",
    "\n",
    "使用 `datasets.load_dataset` 从 Hugging Face Hub 上加载 GLUE/SST-2 任务的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a45464c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/wh/.cache/huggingface/modules/datasets_modules/datasets/glue/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad (last modified on Thu Mar 31 13:53:16 2022) since it couldn't be found locally at glue.\n",
      "Reusing dataset glue (/home/wh/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37a72de877e14e1683a7aa907d3fda4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"glue\", \"sst2\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c98157",
   "metadata": {},
   "source": [
    "对原始数据集分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79daa901",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/wh/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-57abd33ab18d3d8f.arrow\n",
      "Loading cached processed dataset at /home/wh/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-f1701a10d10b29a1.arrow\n",
      "Loading cached processed dataset at /home/wh/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-db86549a2685890a.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\", use_fast=True)\n",
    "\n",
    "def preprocessing(examples):\n",
    "    \"\"\"用于分词的预处理程序\"\"\"\n",
    "    return tokenizer(examples[\"sentence\"], padding=\"max_length\", max_length=60, truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(preprocessing, batched=True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421bbabd",
   "metadata": {},
   "source": [
    "从数据集中取出训练集和验证集，并移除训练过程中不需要的 `sentence` 和 `idx` 字段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7c9d05e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 67349\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = tokenized_datasets[\"train\"].remove_columns([\"sentence\", \"idx\"])\n",
    "eval_dataset = tokenized_datasets[\"validation\"].remove_columns([\"sentence\", \"idx\"])\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587b4fe0",
   "metadata": {},
   "source": [
    "定义 `DataLoader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9045f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=data_collator)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=True, collate_fn=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15867ec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "batch.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e948db81",
   "metadata": {},
   "source": [
    "## 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad391799",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForSST2(nn.Module):\n",
    "    def __init__(self, model_name: str, dropout: float=0.5, use_pooled_output: bool=True):\n",
    "        super(BertForSST2, self).__init__()\n",
    "        self.use_pooled_output = use_pooled_output\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(768, 2),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        if self.use_pooled_output:\n",
    "            cls_representation = outputs[1]\n",
    "        else:\n",
    "            cls_representation = outputs[0][:, 0]\n",
    "        return self.classifier(self.dropout(cls_representation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad6c6084",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertForSST2(\"bert-base-uncased\", use_pooled_output=True)\n",
    "no_pooled_model = BertForSST2(\"bert-base-uncased\", use_pooled_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7465cc95",
   "metadata": {},
   "source": [
    "## 模型微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4b468de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    loss_fn: Callable,\n",
    "    device: str,\n",
    "):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X in dataloader:\n",
    "            X.to(device)\n",
    "            pred = model(X.input_ids, X.attention_mask, X.token_type_ids)\n",
    "            test_loss += loss_fn(pred, X.labels).item()\n",
    "            correct += (pred.argmax(1) == X.labels).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    test_acc = 100 * (correct / size)\n",
    "    return test_loss, test_acc\n",
    "\n",
    "\n",
    "def train_loop(\n",
    "    model: nn.Module, \n",
    "    train_dataloader: DataLoader,\n",
    "    eval_dataloader: DataLoader,\n",
    "    loss_fn: Callable,\n",
    "    optimizer,\n",
    "    lr_scheduler,\n",
    "    device: str,\n",
    "    writer: SummaryWriter,\n",
    "    epoch: int,\n",
    "):\n",
    "    size = len(train_dataloader.dataset)\n",
    "    num_batches = len(train_dataloader)\n",
    "    loop = tqdm(enumerate(train_dataloader), total =len(train_dataloader))\n",
    "    loop.set_description(f'Epoch [{epoch}/{epochs}]')\n",
    "    model.train()\n",
    "    for batch, X in loop:\n",
    "        X.to(device)\n",
    "        # 前向传播并计算loss\n",
    "        pred = model(X.input_ids, X.attention_mask, X.token_type_ids)\n",
    "        loss = loss_fn(pred, X.labels)\n",
    "        # 反向传播并优化模型参数\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # 让进度条显示 acc 和 loss\n",
    "        acc = 100 * (pred.argmax(1) == X.labels).type(torch.float).sum().item() / X.input_ids.size()[0]\n",
    "        loop.set_postfix(loss=loss.item(), acc=acc, lr=optimizer.param_groups[0][\"lr\"])\n",
    "        # 写入 TensorBoard\n",
    "        if batch % 50 == 0:\n",
    "            global_step = epoch * num_batches + batch\n",
    "            test_loss, test_acc = test_loop(model, eval_dataloader, loss_fn, device)\n",
    "            writer.add_scalar(\"Loss/train\", loss.item(), global_step)\n",
    "            writer.add_scalar(\"Acc/train\", acc, global_step)\n",
    "            writer.add_scalar(\"Loss/test\", test_loss, global_step)\n",
    "            writer.add_scalar(\"Acc/test\", test_acc, global_step)\n",
    "            writer.add_scalar(\"Learning rate\", optimizer.param_groups[0][\"lr\"], global_step)\n",
    "            lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0acac983",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "writer = SummaryWriter(f\"./logs/base-base-uncased-sst2-{now}\")\n",
    "no_pooled_writer = SummaryWriter(f\"./logs/base-base-uncased-sst2-no-pooled-{now}\")\n",
    "writer.add_graph(model, [batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"token_type_ids\"]])\n",
    "no_pooled_writer.add_graph(no_pooled_model, [batch[\"input_ids\"], batch[\"attention_mask\"], batch[\"token_type_ids\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fbcbfa8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "791b3e573e4045a4b728ef843c0499e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1053 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc=91.1697 Loss=0.3959\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=.0)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "\n",
    "model.to(device)\n",
    "for t in range(epochs):\n",
    "    train_loop(model, train_dataloader, eval_dataloader, loss_fn, optimizer, lr_scheduler, device, writer, t)\n",
    "    test_loss, test_acc = test_loop(model, eval_dataloader, loss_fn, device)\n",
    "    print(f\"Acc={test_acc:.4f} Loss={test_loss:.4f}\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e72068b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aabe679ca38d4e6481cb63f361c02810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1053 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc=92.3165 Loss=0.3875\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(no_pooled_model.parameters(), lr=learning_rate, weight_decay=.0)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "\n",
    "no_pooled_model.to(device)\n",
    "for t in range(epochs):\n",
    "    train_loop(no_pooled_model, train_dataloader, eval_dataloader, loss_fn, optimizer, lr_scheduler, device, no_pooled_writer, t)\n",
    "    test_loss, test_acc = test_loop(no_pooled_model, eval_dataloader, loss_fn, device)\n",
    "    print(f\"Acc={test_acc:.4f} Loss={test_loss:.4f}\")\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef6401b",
   "metadata": {},
   "source": [
    "## 模型验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f375c1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    device: str,\n",
    "):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X in dataloader:\n",
    "            X.to(device)\n",
    "            pred = model(X.input_ids, X.attention_mask, X.token_type_ids)\n",
    "            y_true.extend(X.labels.tolist())\n",
    "            y_pred.extend(pred.argmax(1).tolist())\n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00417823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[378  50]\n",
      " [ 27 417]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.88      0.91       428\n",
      "    positive       0.89      0.94      0.92       444\n",
      "\n",
      "    accuracy                           0.91       872\n",
      "   macro avg       0.91      0.91      0.91       872\n",
      "weighted avg       0.91      0.91      0.91       872\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true, y_pred = predict(model, eval_dataloader, device)\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print(classification_report(y_true, y_pred, labels=[0, 1], target_names=[\"negative\", \"positive\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8731ecdd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[398  30]\n",
      " [ 37 407]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.93      0.92       428\n",
      "    positive       0.93      0.92      0.92       444\n",
      "\n",
      "    accuracy                           0.92       872\n",
      "   macro avg       0.92      0.92      0.92       872\n",
      "weighted avg       0.92      0.92      0.92       872\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true, y_pred = predict(no_pooled_model, eval_dataloader, device)\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print(classification_report(y_true, y_pred, labels=[0, 1], target_names=[\"negative\", \"positive\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
