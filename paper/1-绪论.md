## 1.1 研究背景与意义
2006年Hinton等人展示了如何有效的训练多层前馈神经网络，自此深度学习成为了机器学习研究中的一个重要领域。随着GPU和其他硬件加速技术的不断进步，深度学习也给世界带来了更多惊喜，推动了计算机视觉、自然语言处理、自动语音识别、强化学习和统计建模等领域的快速发展。近年来，深度学习的研究成果也逐步进入了人们的日常生活，例如门禁系统中的人脸识别模型、在线翻译中的机器翻译模型、手机电脑等智能终端上的AI助理等等。自然语言处理作为人工智能研究的一个重要领域，被誉为“人工智能皇冠上的明珠”，其可以帮助计算机理解、解释和操纵人类语言，致力于填补人类交流与计算机理解之间的沟壑。而自然语言处理也经历了从统计学习到深度学习的发展，现有的自然语言处理模型主要基于深度神经网络。

然而，深度学习模型是一个黑盒模型，因为在建立好一个深度学习模型后，经过充分的训练和测试我们可以得到相当不错的结果，但是我们并不知道为什么它可以表现的这么好。而黑盒模型给模型的可解释性、安全性、公正性等带来了极大的挑战。其中黑盒模型的不安全性问题十分突出。首先对于模型创建者来说，黑盒模型内部结构复杂，当模型受到外界攻击时，通常很难发现这些攻击。其次对于模型使用者来说，他们并不了解模型的运作机制，只是利用模型的结果作出决策，而不了解模型结果可能存在的风险。深度学习模型的安全性引起了人们的广泛关注，很多研究者开始着手于深度学习模型安全性的探索，尝试挖掘深度学习模型中潜在的威胁。

目前，广泛研究的深度学习模型安全威胁主要有对抗样本攻击和后门攻击两大类。其中对抗样本攻击可以简单描述为对输入样本添加一些难以察觉的细微干扰，从而使模型在推理时以很高的置信度给出一个错误的结果。显然该攻击方式主要针对模型的推理阶段。和推理阶段相比，模型的训练阶段包含了更多的步骤，其中有数据收集、数据预处理、模型构建、模型训练、模型存储和模型分发等。更多的步骤意味着攻击者有更多采取行动的机会，所以直观上模型的训练阶段比推理阶段存在更多安全威胁。这些安全威胁可能来自于第三方的数据集、第三方的训练平台或者第三方的模型。后门攻击正是存在于训练阶段的一种威胁，攻击者将后门嵌入到深度学习模型中，使模型在不包含触发器的良性样本上表现和没有嵌入后门的模型相近，而在包含触发器的恶意样本上则给出错误的结果。

古语有云“知彼知己，百战不殆”，对深度学习模型后门攻击的研究可以让人们更深刻的了解到模型存在的安全威胁，从而激发研究着们对防御措施的探索。本研究正是从这个角度出发，研究并实现自然语言处理模型的后门攻击，找出其中潜在的安全威胁。

## 1.2 国内外研究现状


## 1.3 本文主要研究内容和创新点


## 1.4 论文结构安排
